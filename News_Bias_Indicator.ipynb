{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07b1749-6fe2-4838-8a1b-757f76454f7d",
   "metadata": {},
   "source": [
    "# News Bias Indicator\n",
    "This notebook applies the PyTorch classification model created in the News_Bias_Classification_Model notebook to classify the political bias of a provided article. Then, after predicting the political bias, this notebook utilizes SerpAPI to webscrape and provide articles covering the same topic with similar and different political biases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec3e33-67b9-46f7-b5a2-0b74fbda4206",
   "metadata": {},
   "source": [
    "# 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15632c7b-b13f-4b37-b3b7-e627fc07bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from serpapi import GoogleSearch\n",
    "from urllib.parse import urlparse\n",
    "from itables import init_notebook_mode\n",
    "init_notebook_mode(all_interactive=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51905070-fe99-40aa-a14b-cac8e20ea873",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"Left\", 1: \"Center\", 2: \"Right\"}\n",
    "blocked_websites = 'https://www.wsj.com/'\n",
    "diff_HTML = ['reuters.com']\n",
    "popup_sites = ['cnn.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee11f2-76f5-410b-ba48-f4589b8aae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust Input URL to desired article\n",
    "input_url = 'https://www.theguardian.com/business/2025/sep/05/us-jobs-report-august-tariffs'\n",
    "# Adjust API Key\n",
    "api_key = 'Paste SERPAPI key here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9dddb5-06d0-4fb3-aa69-644b0583b0b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2fa2d8-0183-499c-b464-eb834d3deb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(url, page_load_timeout=60, element_wait_timeout=30):\n",
    "    \"\"\"\n",
    "    Extract title and article content from a URL safely.\n",
    "    Relies on Selenium timeouts to prevent hanging.\n",
    "    \n",
    "    Parameters:\n",
    "    - url: str, the URL to scrape\n",
    "    - page_load_timeout: int, max seconds to wait for page to load\n",
    "    - element_wait_timeout: int, max seconds to wait for elements to appear\n",
    "    \"\"\"\n",
    "    driver = None\n",
    "    try:\n",
    "        exception = any(key.lower() in url.lower() for key in diff_HTML)\n",
    "        popup_exists = any(key.lower() in url.lower() for key in popup_sites)\n",
    "        \n",
    "        # Initialize Chrome\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "        driver.set_page_load_timeout(page_load_timeout)\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Handle pop-up if it exists\n",
    "        if popup_exists:\n",
    "            try:\n",
    "                WebDriverWait(driver, element_wait_timeout).until(\n",
    "                    EC.element_to_be_clickable((By.ID, \"close-pc-btn-handler\"))\n",
    "                ).click()\n",
    "            except (TimeoutException, NoSuchElementException):\n",
    "                try:\n",
    "                    WebDriverWait(driver, element_wait_timeout).until(\n",
    "                        EC.element_to_be_clickable((By.CLASS_NAME, \"onetrust-close-btn-handler\"))\n",
    "                    ).click()\n",
    "                except (TimeoutException, NoSuchElementException):\n",
    "                    pass  # Pop-up not present, continue\n",
    "        \n",
    "        # Extract title safely\n",
    "        try:\n",
    "            title = WebDriverWait(driver, element_wait_timeout).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"h1\"))\n",
    "            ).text\n",
    "        except TimeoutException:\n",
    "            title = None\n",
    "        \n",
    "        # Extract content\n",
    "        content = \"\"\n",
    "        if exception:\n",
    "            paragraph_divs = driver.find_elements(By.XPATH, \"//div[starts-with(@data-testid,'paragraph-')]\")\n",
    "            content = \" \".join([div.text for div in paragraph_divs])\n",
    "        else:\n",
    "            paragraphs = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "            content = \" \".join([p.text for p in paragraphs])\n",
    "        \n",
    "        return pd.DataFrame([{\"url\": url, \"title\": title, \"content\": content}])\n",
    "    \n",
    "    except (TimeoutException, WebDriverException) as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return pd.DataFrame([{\"url\": url, \"title\": None, \"content\": None}])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {url}: {e}\")\n",
    "        return pd.DataFrame([{\"url\": url, \"title\": None, \"content\": None}])\n",
    "    \n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa69ce-c289-43f2-aac7-c6765158adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bias(texts, model, tokenizer, label_map=None):\n",
    "    \"\"\"\n",
    "    Apply classification model to articles to determine bias\n",
    "    \"\"\"\n",
    "    texts = [\"\" if t is None else str(t) for t in texts]\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # keep only the args that the model forward() actually supports\n",
    "    valid_keys = model.forward.__code__.co_varnames\n",
    "    inputs = {k: v for k, v in inputs.items() if k in valid_keys}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1).tolist()\n",
    "    \n",
    "    if label_map:\n",
    "        return [label_map[i] for i in pred_ids]\n",
    "    return pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d8759-dee1-44cc-9313-6f7b4106c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_search(search_query, api_key, df, num_results=250, max_unique=14):\n",
    "    \"\"\"\n",
    "    Search Google News with SerpAPI for articles on the same event.\n",
    "    Returns a list of links from unique publishers (domains), excluding blocked websites\n",
    "    and the domain of the first URL in df.\n",
    "    \n",
    "    Parameters:\n",
    "    - search_query: string, the query to search\n",
    "    - api_key: string, SerpAPI key\n",
    "    - df: pandas DataFrame with at least a 'url' column\n",
    "    - num_results: int, number of results to request\n",
    "    - max_unique: int, maximum number of unique links to return\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"q\": search_query,\n",
    "        \"tbm\": \"nws\",\n",
    "        \"location\": \"United States\",\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "        \"google_domain\": \"google.com\",\n",
    "        \"num\": num_results,\n",
    "        \"api_key\": api_key,\n",
    "        \"filter\": 0\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "\n",
    "    links = []\n",
    "    seen_domains = set()\n",
    "\n",
    "    # Add domain of first URL in df\n",
    "    first_url_domain = urlparse(df['url'].iloc[0]).netloc\n",
    "    seen_domains.add(first_url_domain)\n",
    "\n",
    "    # Add blocked website domains\n",
    "    seen_domains.add(urlparse(blocked_websites).netloc)\n",
    "\n",
    "    # Collect unique links\n",
    "    if \"news_results\" in results:\n",
    "        for res in results[\"news_results\"]:\n",
    "            if \"link\" in res:\n",
    "                domain = urlparse(res[\"link\"]).netloc\n",
    "                if domain not in seen_domains:\n",
    "                    links.append(res[\"link\"])\n",
    "                    seen_domains.add(domain)\n",
    "                if len(links) >= max_unique:\n",
    "                    break\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498cd127-1c38-452c-81b6-b2741483c80b",
   "metadata": {},
   "source": [
    "# 2: Webscrape Input Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9b8df8-d441-40a5-a23a-c3165286338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_article = scrape_article(input_url)\n",
    "input_article['text']  = input_article['title'] + \" \" + input_article['content']\n",
    "X_articles = input_article['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30edc140-fdd2-4340-a4d4-c077804a79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your saved model and tokenizer\n",
    "save_path = \"C:/Users/mouct/Downloads/best_bias_model\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "predictions = predict_bias(X_articles, model, tokenizer, label_map)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b1133-3d7e-458c-a6a2-ff61c7f1deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_article['bias'] = predictions\n",
    "print(input_article[['url','title','bias']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4588d46-c133-4c6c-9ec8-4cdca0e4c2a7",
   "metadata": {},
   "source": [
    "# 3: Webscrape Alternative/Similar Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34362d37-ef84-4acc-984a-82cd25f6f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the title into words, take the first 8, and join them back\n",
    "first_8_words = \" \".join(input_article['title'].iloc[0].split()[:7])\n",
    "search_query = first_8_words + \" coverage\"\n",
    "current_bias = input_article['bias'].iloc[0]\n",
    "print(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f4448-30c5-4fbd-9c67-4f3b10b7df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = google_search(search_query, api_key)\n",
    "\n",
    "print(\"Search results:\")\n",
    "for i, link in enumerate(links, 1):\n",
    "    print(f\"{i}. {link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2203d-4f0c-4b20-8cb2-59cc3ab8454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for link in links:\n",
    "    dfs.append(scrape_article(link))\n",
    "alternative_articles = pd.concat(dfs, ignore_index=True)\n",
    "alternative_articles['text'] = (alternative_articles['title'].fillna('') + \" \" + alternative_articles['content'].fillna(''))\n",
    "X_articles = alternative_articles['text'].astype(str).tolist()\n",
    "predictions = predict_bias(X_articles, model, tokenizer, label_map)\n",
    "alternative_articles['bias'] = predictions\n",
    "alternative_articles.dropna(inplace=True)\n",
    "similar_articles = alternative_articles[alternative_articles['bias'] == current_bias]\n",
    "alternative_articles = alternative_articles[\n",
    "    (alternative_articles['bias'] != current_bias) & \n",
    "    (alternative_articles['bias'].notna())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d9c8f-a651-45c1-a203-ca72348b6518",
   "metadata": {},
   "source": [
    "# 4: Showcase All Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5d9f2-7a11-40d4-81d1-af7dc1740d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Article\n",
    "print(\"**Input Article:\")\n",
    "print('Title: ', input_article['title'].iloc[0])\n",
    "print('URL: ', input_article['url'].iloc[0])\n",
    "print('Bias: ', input_article['bias'].iloc[0])\n",
    "print('')\n",
    "\n",
    "# Blindspots:\n",
    "print(\"**Blindspot Article(s):\")\n",
    "for i in range(len(alternative_articles)):\n",
    "    print(\"Title: \", alternative_articles['title'].iloc[i])\n",
    "    print(\"URL: \", alternative_articles['url'].iloc[i])\n",
    "    print(\"Bias: \", alternative_articles['bias'].iloc[i])\n",
    "    print('')\n",
    "\n",
    "# Similar Bias:\n",
    "print(\"**Similar Article(s):\")\n",
    "for i in range(len(similar_articles)):\n",
    "    print(\"Title: \", similar_articles['title'].iloc[i])\n",
    "    print(\"URL: \", similar_articles['url'].iloc[i])\n",
    "    print(\"Bias: \", similar_articles['bias'].iloc[i])\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clean_env]",
   "language": "python",
   "name": "conda-env-clean_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
